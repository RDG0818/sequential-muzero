Ray cluster started. Available resources: {'CPU': 12.0, 'node:172.23.48.184': 1.0, 'object_store_memory': 4068151296.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'memory': 9492353024.0}
Waiting for actors to initialize and run one episode...
[36m(LearnerActor pid=44987)[0m (Learner pid=44987) Initializing on GPU...
[36m(LearnerActor pid=44987)[0m (Learner pid=44987) Setup complete.

Warmup phase...
  Buffer size: 1000/1000
Warmup complete. Time Taken 1.00 seconds. Starting main training loop.
[36m(DataActor pid=44982)[0m [32m2025-06-29 16:51:53[0m | [36mDEBUG[0m | [36mUpdating parameters from learner.[0m
[36m(DataActor pid=44982)[0m [32m2025-06-29 16:52:10[0m | [36mDEBUG[0m | [36mUpdating parameters from learner.[0m[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
Episodes: 100 | Avg Return: -87.68 | Avg Loss: 47.0309 | Time: 17.44s
Episodes: 200 | Avg Return: -88.91 | Avg Loss: 43.1829 | Time: 1.96s
Episodes: 300 | Avg Return: -88.15 | Avg Loss: 42.0394 | Time: 1.85s
[36m(DataActor pid=44982)[0m [32m2025-06-29 16:52:15[0m | [36mDEBUG[0m | [36mUpdating parameters from learner.[0m[32m [repeated 30x across cluster][0m
Episodes: 400 | Avg Return: -88.14 | Avg Loss: 40.7714 | Time: 1.94s
Episodes: 500 | Avg Return: -86.37 | Avg Loss: 39.5861 | Time: 2.05s
Episodes: 600 | Avg Return: -98.52 | Avg Loss: 38.9884 | Time: 1.91s
[36m(DataActor pid=44982)[0m [32m2025-06-29 16:52:21[0m | [36mDEBUG[0m | [36mUpdating parameters from learner.[0m[32m [repeated 30x across cluster][0m
Episodes: 700 | Avg Return: -88.88 | Avg Loss: 37.8998 | Time: 2.01s
Episodes: 800 | Avg Return: -94.73 | Avg Loss: 37.1505 | Time: 2.05s
Episodes: 900 | Avg Return: -82.11 | Avg Loss: 35.7006 | Time: 2.09s
[36m(DataActor pid=44986)[0m [32m2025-06-29 16:52:26[0m | [36mDEBUG[0m | [36mUpdating parameters from learner.[0m[32m [repeated 27x across cluster][0m
Episodes: 1000 | Avg Return: -98.48 | Avg Loss: 34.3618 | Time: 2.62s
Episodes: 1100 | Avg Return: -95.18 | Avg Loss: 33.7349 | Time: 2.02s
[36m(DataActor pid=44982)[0m [32m2025-06-29 16:52:32[0m | [36mDEBUG[0m | [36mUpdating parameters from learner.[0m[32m [repeated 21x across cluster][0m
Episodes: 1200 | Avg Return: -95.34 | Avg Loss: 33.8372 | Time: 1.73s
Episodes: 1300 | Avg Return: -88.07 | Avg Loss: 33.6020 | Time: 1.71s
Episodes: 1400 | Avg Return: -85.43 | Avg Loss: 34.2136 | Time: 1.73s
[36m(DataActor pid=44982)[0m [32m2025-06-29 16:52:37[0m | [36mDEBUG[0m | [36mUpdating parameters from learner.[0m[32m [repeated 30x across cluster][0m
Traceback (most recent call last):
  File "/home/ryan/toy_mazero/train.py", line 467, in <module>
  File "/home/ryan/toy_mazero/train.py", line 422, in main
    done_actor_ref = done_actor_refs[0]
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/worker.py", line 3080, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/ryan/toy_mazero/train.py", line 467, in <module>
  File "/home/ryan/toy_mazero/train.py", line 422, in main
    done_actor_ref = done_actor_refs[0]
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/worker.py", line 3080, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7b2a74751a20>
Traceback (most recent call last):
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 90, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 218, in teardown
    self._router.join()
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
[0m
Exception ignored in atexit callback: <function shutdown at 0x7b2a974b5630>
Traceback (most recent call last):
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/worker.py", line 1986, in shutdown
    time.sleep(0.5)
KeyboardInterrupt:
