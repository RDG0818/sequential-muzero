_wandb:
    value:
        cli_version: 0.20.1
        m: []
        python_version: 3.10.18
        t:
            "1":
                - 30
            "2":
                - 12
                - 30
                - 45
            "3":
                - 16
                - 55
            "4": 3.10.18
            "5": 0.20.1
            "12": 0.20.1
            "13": linux-x86_64
mcts:
    value:
        max_depth_gumbel_search: 10
        num_gumbel_samples: 10
        num_joint_samples: 16
        num_simulations: 50
        planner_mode: joint
model:
    value:
        attention_heads: 4
        attention_layers: 3
        attention_type: none
        dropout_rate: 0.1
        fc_dynamic_layers:
            - 128
        fc_policy_layers:
            - 32
        fc_representation_layers:
            - 128
        fc_reward_layers:
            - 32
        fc_value_layers:
            - 32
        hidden_state_size: 128
        pred_hid: 64
        pred_out: 256
        proj_hid: 256
        proj_out: 256
        reward_support_size: 300
        value_support_size: 300
train:
    value:
        batch_size: 256
        consistency_coeff: 1
        discount_gamma: 0.99
        end_lr_factor: 0.1
        env_name: MPE_simple_spread_v3
        gradient_clip_norm: 5
        learning_rate: 0.0001
        log_interval: 100
        lr_warmup_steps: 5000
        max_episode_steps: 25
        n_step: 1
        num_actors: 6
        num_agents: 3
        num_episodes: 500000
        param_update_interval: 10
        replay_buffer_alpha: 0.6
        replay_buffer_beta_frames: 100000
        replay_buffer_beta_start: 0.4
        replay_buffer_size: 20000
        unroll_steps: 5
        value_loss_coefficient: 1
        warmup_episodes: 1000
