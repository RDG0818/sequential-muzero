Ray cluster started. Available resources: {'GPU': 1.0, 'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'node:172.23.48.184': 1.0, 'memory': 10214213632.0, 'object_store_memory': 4377520128.0}
Waiting for actors to initialize and run one episode...
[36m(DataActor pid=2090)[0m (DataActor pid=2090) Initializing on CPU...
[36m(DataActor pid=2090)[0m (DataActor pid=2090) Setup complete.
[36m(LearnerActor pid=2092)[0m (Learner pid=2092) Initializing on GPU...
[36m(LearnerActor pid=2092)[0m (Learner pid=2092) Setup complete.
[36m(DataActor pid=2093)[0m (DataActor pid=2093) Initializing on CPU...[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(DataActor pid=2096)[0m (DataActor pid=2096) Setup complete.[32m [repeated 5x across cluster][0m

Warmup phase...
  Buffer size: 1000/1000
Warmup complete. Starting main training loop.
Episodes: 1100 | Avg Return: -1351.78 | Avg Loss: 403.6341 | Time: 66.62s
Episodes: 1200 | Avg Return: -1343.89 | Avg Loss: 403.0161 | Time: 54.91s
Traceback (most recent call last):
  File "/home/ryan/toy_mazero/train.py", line 102, in <module>
    main()
  File "/home/ryan/toy_mazero/train.py", line 58, in main
    done_actor_refs, _ = ray.wait(list(actor_tasks.keys()), num_returns=1)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/worker.py", line 3080, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
  File "python/ray/_raylet.pyx", line 3480, in ray._raylet.CoreWorker.wait
  File "python/ray/includes/common.pxi", line 83, in ray._raylet.check_status
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/ryan/toy_mazero/train.py", line 102, in <module>
    main()
  File "/home/ryan/toy_mazero/train.py", line 58, in main
    done_actor_refs, _ = ray.wait(list(actor_tasks.keys()), num_returns=1)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/ray/_private/worker.py", line 3080, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
  File "python/ray/_raylet.pyx", line 3480, in ray._raylet.CoreWorker.wait
  File "python/ray/includes/common.pxi", line 83, in ray._raylet.check_status
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x71266c5e2f80>
Traceback (most recent call last):
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 90, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/wandb/sdk/lib/service_connection.py", line 218, in teardown
    self._router.join()
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/ryan/miniconda3/envs/jaxmarl_env/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
[0m
