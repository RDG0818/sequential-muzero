_wandb:
    value:
        cli_version: 0.20.1
        m: []
        python_version: 3.10.18
        t:
            "1":
                - 30
            "2":
                - 30
            "3":
                - 16
                - 55
                - 61
            "4": 3.10.18
            "5": 0.20.1
            "12": 0.20.1
            "13": linux-x86_64
batch_size:
    value: 256
discount_gamma:
    value: 0.99
env_name:
    value: MPE_simple_spread_v3
fc_dynamic_layers:
    value:
        - 128
fc_policy_layers:
    value:
        - 32
fc_representation_layers:
    value:
        - 128
fc_reward_layers:
    value:
        - 32
fc_value_layers:
    value:
        - 32
hidden_state_size:
    value: 128
learning_rate:
    value: 0.0001
log_interval:
    value: 100
max_depth_gumbel_search:
    value: 10
max_episode_steps:
    value: 100
num_actors:
    value: 6
num_agents:
    value: 3
num_episodes:
    value: 100000
num_gumbel_samples:
    value: 10
num_joint_samples:
    value: 16
num_simulations:
    value: 50
planner_mode:
    value: joint
replay_buffer_size:
    value: 20000
reward_support_size:
    value: 300
unroll_steps:
    value: 5
value_support_size:
    value: 300
warmup_episodes:
    value: 1000
