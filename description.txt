Description of major design choices of this MARL Algorithm:

Written in the JAX/FLAX framework for JIT compilation and XLA support
Model-based, similar to MuZero for multi-agent reinforcement learning environments

Utilizes a representation, dynamics, and prediction network for the world model
Dynamics network has an optional transformer-style attention mechanism for communication
Utilizes scalar to categorical distribution transforms for reward and value network training stability

Uses the MCTX library for MuZero and Gumbel MuZero functions
Split between independent (per agent) MCTS and joint MCTS

Asynchronous training, similar to EfficientZero, using Ray framework 
Multiple DataActors on CPU, single LearnerActor on GPU
Loss for each network is calculated over k unroll steps, n-step returns for value
Prioritized Experience Replay Buffer
Utilizes LR scheduler, adamw optimizer, gradient clipping, and loss scaling for training stability
Utilizes jaxMARL for jax environments
Uses wandb for logging

Basic unit testing for all major files 

To Add:
add type hinting, proper commenting, switch to logging/coloring, etc.
world model pretraining
Projection network for gradient shielding and training stability
model saving/checkpointing/testing/rendering
Add more unit tests, clean up existing unit tests, use logging module for unit tests

Data from run_episodes -> process episodes -> replay buffer -> train_step/loss_fn

automated hyperparameter tuning
direchelet noise to root
include state in replay buffer, more metrics return from run_episode
Conditioned/sequential/alternating MCTS idea
Tuning and variations of attention mechanism
Switch to vmap and end-to-end JAX for DataActors
Possibly utilize legal actions if provided?
Possibly switch to hydra for hyperparmeters?
Possibly add a network to create global state out of individual latent states?
