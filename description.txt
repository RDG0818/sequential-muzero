Description of major design choices of this MARL Algorithm:

Written in the JAX/FLAX framework for JIT compilation and XLA support
Model-based, similar to MuZero for multi-agent reinforcement learning environments

Utilizes a representation, dynamics, and prediction network for the world model
Utilizes a projection network to force the representation/dynamics network to learn a consistent hidden state
Dynamics network has an optional transformer-style attention mechanism for communication
Utilizes scalar to categorical distribution transforms for reward and value network training stability

Uses the MCTX library for MuZero and Gumbel MuZero functions
Split between independent (per agent) MCTS and joint MCTS

Asynchronous training, similar to EfficientZero, using Ray framework 
Multiple DataActors on CPU, single LearnerActor on GPU
Loss for each network is calculated over k unroll steps, n-step returns for value
Prioritized Experience Replay Buffer
Utilizes LR scheduler, adamw optimizer, gradient clipping, and loss scaling for training stability
Utilizes jaxMARL for jax environments
Uses wandb for logging

To Add:
add type hinting, proper commenting, switch to logging/coloring, etc.
world model pretraining
model saving/checkpointing/testing/rendering
Add more unit tests, clean up existing unit tests, use logging module for unit tests

Data from run_episodes -> process episodes -> replay buffer -> train_step/loss_fn

automated hyperparameter tuning
evaluation script
direchelet noise to root
include state in replay buffer, more metrics return from run_episode
Conditioned/sequential/alternating MCTS idea
Tuning and variations of attention mechanism

Possible Future Addtions:
Dreamer style dynamics?
Better attention mechanisms?
Value decomposition?
Stochastic World Model?
GNNs for coordination?
Communication protocols?
Switch to vmap and end-to-end JAX for DataActors
Legal Actions?
Hydra?
Possibly add a network to create global state out of individual latent states?
